{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val PATH = \"file:///Users/lzz/work/SparkML/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.wholeTextFiles( PATH + \"data/20news-bydate-train/*\" )\n",
    "val text = rdd.map{ case( file, text ) => text }\n",
    "println( text.count )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rec.sport.hockey,600)\n",
      "(soc.religion.christian,599)\n",
      "(rec.motorcycles,598)\n",
      "(rec.sport.baseball,597)\n",
      "(sci.crypt,595)\n",
      "(rec.autos,594)\n",
      "(sci.med,594)\n",
      "(comp.windows.x,593)\n",
      "(sci.space,593)\n",
      "(sci.electronics,591)\n",
      "(comp.os.ms-windows.misc,591)\n",
      "(comp.sys.ibm.pc.hardware,590)\n",
      "(misc.forsale,585)\n",
      "(comp.graphics,584)\n",
      "(comp.sys.mac.hardware,578)\n",
      "(talk.politics.mideast,564)\n",
      "(talk.politics.guns,546)\n",
      "(alt.atheism,480)\n",
      "(talk.politics.misc,465)\n",
      "(talk.religion.misc,377)\n"
     ]
    }
   ],
   "source": [
    "val newsgroups = rdd.map{ case (file, text) => file.split(\"/\").takeRight(2).head }\n",
    "val countByGroup = newsgroups.map( n => (n, 1)).reduceByKey(_+_).collect.sortBy(-_._2).mkString(\"\\n\")\n",
    "println( countByGroup )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402978\n"
     ]
    }
   ],
   "source": [
    "val text = rdd.map { case (file, text) => text }\n",
    "val whiteSpaceSplit = text.flatMap( t => t.split(\" \").map(_.toLowerCase) )\n",
    "println( whiteSpaceSplit.distinct.count )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from:,mathew,mathew,faq:,faq:,atheist,resources\n",
      "summary:,music,--,fiction,,mantis,consultants,,uk.\n",
      "supersedes:,290\n",
      "\n",
      "archive-name:,1.0\n",
      "\n",
      ",,,,,,,,,,,,,,,,,,,organizations\n",
      "\n",
      ",organizations\n",
      "\n",
      ",,,,,,,,,,,,,,,,stickers,and,and,the,from,from,in,to:,to:,ffrf,,256-8900\n",
      "\n",
      "evolution,designs\n",
      "\n",
      "evolution,a,stick,cars,,written\n",
      "inside.,fish,us.\n",
      "\n",
      "write,evolution,,,,,,,bay,can,get,get,,to,the\n",
      "price,is,of,the,the,so,on.,and,foote.,,atheist,pp.,0-910309-26-4,,,atrocities,,foote:,aap.,,the\n"
     ]
    }
   ],
   "source": [
    "println( whiteSpaceSplit.sample(true, 0.3, 42).take(100).mkString(\",\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130126\n"
     ]
    }
   ],
   "source": [
    "val nonWordSplit = text.flatMap( t => t.split( \"\"\"\\W+\"\"\" ).map( _.toLowerCase))\n",
    "println( nonWordSplit.distinct.count )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kv07,jejones,jejones,ml5,ml5,w1w3s1,bone,k29p,schwabam,chipset,tenex,hcq,entitlements,he3,neurologists,jxicaijp,regina,regina,b0vp,c1381,adultery,fzbv1u,c1,c1,ao,wargame,nonmeasurable,391k,seetex,viewed,unforeseen,eur,m2ffjk,strut,strut,tic,wp3d,randall_clark,mswin,cannibal,searflame,34ij,13963,13963,siumv,right,z0ozk,z0ozk,g85,93864,igpp,ulcerative,ulcerative,remailing,012537,012537,f80,xs9,xs9,detergent,aanp,rlg1,robert,deterministic,rockefeller,hour,scramblers,shutdown,1r1d62,mtearle,exhausting,discernible,siiafeid8,qnh1,qnh1,paradijs,abstract,crudely,crudely,tripe,chama,triangulate,iaik,dvbtpuc,herod,herod,nutty,homerific,phoniest,uflkll_00vpcekw15e,transylvania,pdp11,liberated,canonical,c2xjfa,birds,xtappcontext,lamers,8v0,b4r\n"
     ]
    }
   ],
   "source": [
    "println( nonWordSplit.distinct.sample( true, 0.3, 42 ).take(100).mkString(\",\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84912\n"
     ]
    }
   ],
   "source": [
    "val regex = \"\"\"[^0-9]*\"\"\".r\n",
    "val filterNumbers = nonWordSplit.filter(token => regex.pattern.matcher(token).matches )\n",
    "println( filterNumbers.distinct.count )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rexlex,jejones,jejones,silikian,silikian,reunion,wuair,schwabam,dpsi,bruns,breath,gottschalk,semites,fowl,afterward,ignore,upo,upo,mowtu,arius,jbis,upsets,aces,aces,majorly,akl,underscored,steaminess,sively,jayson,qsins,historians,initiation,krantz,krantz,jmckinney,nonmeasurable,isv,bellevue,seetex,kjvar,rolled,wakaluk,wakaluk,foghorns,lapsing,tinuing,tinuing,croissant,readjoy,comparing,bippy,bippy,sophia,jaze,jaze,mswin,theoreticians,theoreticians,enlarge,decriminalize,right,vied,internship,keysym,igpp,handful,dtn,nixdorf,miserable,brow,icbz,colina,poking,poking,inre,sjoberg,computational,computational,df,emstation,inviting,jkis_ltd,santiago,mishandles,mishandles,anachronistic,springer,hfd,sublingual,vow,nowadays,multiway,formac,altenhofen,responsbible,fuenfzig,trial,rluap,crudely\n"
     ]
    }
   ],
   "source": [
    "println( filterNumbers.distinct.sample( true, 0.3, 42 ).take(100).mkString(\",\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(the,146532)\n",
      "(to,75064)\n",
      "(of,69034)\n",
      "(a,64195)\n",
      "(ax,62406)\n",
      "(and,57957)\n",
      "(i,53036)\n",
      "(in,49402)\n",
      "(is,43480)\n",
      "(that,39264)\n",
      "(it,33638)\n",
      "(for,28600)\n",
      "(you,26682)\n",
      "(from,22670)\n",
      "(s,22337)\n",
      "(edu,21321)\n",
      "(on,20493)\n",
      "(this,20121)\n",
      "(be,19285)\n",
      "(t,18728)\n"
     ]
    }
   ],
   "source": [
    "val tokenCounts = filterNumbers.map( t => (t, 1)).reduceByKey(_+_)\n",
    "val oreringDesc = Ordering.by[(String, Int), Int](_._2)\n",
    "println( tokenCounts.top(20)(oreringDesc).mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ax,62406)\n",
      "(i,53036)\n",
      "(you,26682)\n",
      "(s,22337)\n",
      "(edu,21321)\n",
      "(t,18728)\n",
      "(m,12756)\n",
      "(subject,12264)\n",
      "(com,12133)\n",
      "(lines,11835)\n",
      "(can,11355)\n",
      "(organization,11233)\n",
      "(re,10534)\n",
      "(what,9861)\n",
      "(there,9689)\n",
      "(x,9332)\n",
      "(all,9310)\n",
      "(will,9279)\n",
      "(we,9227)\n",
      "(one,9008)\n"
     ]
    }
   ],
   "source": [
    "val stopwords = Set( \"the\",\"a\",\"an\",\"of\",\"or\",\"in\",\"for\",\"by\",\"on\",\"but\",\"is\",\"not\",\n",
    "\"with\",\"as\",\"was\",\"if\",\"they\",\"are\",\"this\",\"and\",\"it\",\"have\",\"from\",\"at\",\"my\",\"be\",\"that\",\"to\")\n",
    "val tokenCountsFilteredStopwords = tokenCounts.filter{ case\n",
    "(k,v) => !stopwords.contains(k) }\n",
    "println( tokenCountsFilteredStopwords.top(20)(oreringDesc).mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ax,62406)\n",
      "(you,26682)\n",
      "(edu,21321)\n",
      "(subject,12264)\n",
      "(com,12133)\n",
      "(lines,11835)\n",
      "(can,11355)\n",
      "(organization,11233)\n",
      "(re,10534)\n",
      "(what,9861)\n",
      "(there,9689)\n",
      "(all,9310)\n",
      "(will,9279)\n",
      "(we,9227)\n",
      "(one,9008)\n",
      "(would,8905)\n",
      "(do,8674)\n",
      "(he,8441)\n",
      "(about,8336)\n",
      "(writes,7844)\n"
     ]
    }
   ],
   "source": [
    "val tokenCountsFilteredSize = tokenCountsFilteredStopwords.filter{ case(k, v) => k.size >= 2 }\n",
    "println( tokenCountsFilteredSize.top(20)(oreringDesc).mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(altina,1)\n",
      "(bluffing,1)\n",
      "(preload,1)\n",
      "(lennips,1)\n",
      "(actu,1)\n",
      "(vno,1)\n",
      "(wbp,1)\n",
      "(donnalyn,1)\n",
      "(ydag,1)\n",
      "(mirosoft,1)\n",
      "(jjjjrw,1)\n",
      "(harger,1)\n",
      "(conts,1)\n",
      "(bankruptcies,1)\n",
      "(uncompression,1)\n",
      "(d_nibby,1)\n",
      "(bunuel,1)\n",
      "(odf,1)\n",
      "(swith,1)\n",
      "(pacified,1)\n"
     ]
    }
   ],
   "source": [
    "val oreringAsc = Ordering.by[(String, Int), Int](-_._2)\n",
    "println(tokenCountsFilteredSize.top(20)(oreringAsc).mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(sina,2)\n",
      "(akachhy,2)\n",
      "(mvd,2)\n",
      "(sarkis,2)\n",
      "(wendel_clark,2)\n",
      "(relieves,2)\n",
      "(purposeful,2)\n",
      "(hizbolah,2)\n",
      "(wout,2)\n",
      "(uneven,2)\n",
      "(senna,2)\n",
      "(subdivided,2)\n",
      "(bushy,2)\n",
      "(feagans,2)\n",
      "(coretest,2)\n",
      "(oww,2)\n",
      "(historicity,2)\n",
      "(mmg,2)\n",
      "(margitan,2)\n",
      "(defiance,2)\n"
     ]
    }
   ],
   "source": [
    "val rareTokens = tokenCounts.filter{ case (k, v) => v < 2 }.map{ case (k, v) => k }.collect.toSet\n",
    "val tokenCountsFilteredAll = tokenCountsFilteredSize.filter{ case(k, v) => !rareTokens.contains(k) }\n",
    "println( tokenCountsFilteredAll.top(20)(oreringAsc).mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51801\n"
     ]
    }
   ],
   "source": [
    "println( tokenCountsFilteredAll.count )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51801\n"
     ]
    }
   ],
   "source": [
    "def tokenize( line: String ): Seq[String] = {\n",
    "    line.split(\"\"\"\\W+\"\"\")\n",
    "    .map( _.toLowerCase )\n",
    "    .filter( token => regex.pattern.matcher(token).matches )\n",
    "    .filterNot( token => stopwords.contains(token) )\n",
    "    .filterNot( token => rareTokens.contains(token) )\n",
    "    .filter( token => token.size >= 2 )\n",
    "    .toSeq\n",
    "}\n",
    "println( text.flatMap(doc => tokenize(doc)).distinct.count )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WrappedArray(mathew, mathew, mantis, co, uk, subject, alt, atheism, faq, atheist, resources, summary, books, addresses, music, anything, related, atheism, keywords, faq)\n"
     ]
    }
   ],
   "source": [
    "val tokens = text.map( doc => tokenize(doc))\n",
    "println( tokens.first.take(20) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练TF－IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[50] at map at HashingTF.scala:78"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg.{ SparseVector => SV }\n",
    "import org.apache.spark.mllib.feature.HashingTF\n",
    "import org.apache.spark.mllib.feature.IDF\n",
    "val dim = math.pow(2, 18).toInt\n",
    "val hashingTF = new HashingTF(dim)\n",
    "val tf = hashingTF.transform( tokens )\n",
    "tf.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262144\n",
      "706\n",
      "WrappedArray(1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0)\n",
      "WrappedArray(313, 713, 871, 1202, 1203, 1209, 1795, 1862, 3115, 3166)\n"
     ]
    }
   ],
   "source": [
    "val v = tf.first.asInstanceOf[SV]\n",
    "println(v.size)\n",
    "println( v.values.size)\n",
    "println( v.values.take(10).toSeq )\n",
    "println( v.indices.take(10).toSeq )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "706\n",
      "WrappedArray(2.3869085659322193, 4.670445463955571, 6.561295835827856, 4.597686109673142, 8.932700215224111, 5.750365619611528, 2.1871123786150006, 5.520408782213984, 3.4312512246662714, 1.7430324343790569)\n",
      "WrappedArray(313, 713, 871, 1202, 1203, 1209, 1795, 1862, 3115, 3166)\n"
     ]
    }
   ],
   "source": [
    "val idf = new IDF().fit(tf)\n",
    "val tfidf = idf.transform(tf)\n",
    "val v2 = tfidf.first.asInstanceOf[SV]\n",
    "println( v2.values.size )\n",
    "println( v2.values.take(10).toSeq )\n",
    "println( v2.indices.take(10).toSeq )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0,66155.39470409753)\n"
     ]
    }
   ],
   "source": [
    "val minMaxVals = tfidf.map{ v =>\n",
    "    val sv = v.asInstanceOf[SV]\n",
    "    (sv.values.min, sv.values.max)\n",
    "}\n",
    "val globalMinMax = minMaxVals.reduce{ case ( (min1, max1),(min2, max2)) => (math.min(min1, min2), math.max(max1, max2))}\n",
    "println( globalMinMax )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WrappedArray(0.9965359935704624, 1.3348773448236835, 0.5457486182039175)\n"
     ]
    }
   ],
   "source": [
    "val common = sc.parallelize( Seq( Seq(\"you\", \"do\",\"we\")) )\n",
    "val tfCommon = hashingTF.transform(common)\n",
    "val tfidfCommon = idf.transform( tfCommon )\n",
    "val commonVector = tfidfCommon.first.asInstanceOf[SV]\n",
    "println(commonVector.values.toSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WrappedArray(5.3265513728351666, 5.308532867332488, 5.483736956357579)\n"
     ]
    }
   ],
   "source": [
    "val uncommon = sc.parallelize( Seq( Seq(\"telescope\",\"legislation\",\"investment\") ) )\n",
    "val tfUncommon = hashingTF.transform( uncommon )\n",
    "val tfidfUncommon = idf.transform( tfUncommon )\n",
    "val uncommonVector = tfidfUncommon.first.asInstanceOf[SV]\n",
    "println( uncommonVector.values.toSeq )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  使用TF-IDF模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val hockeyText = rdd.filter{ case( file, text) => file.contains(\"hockey\") }\n",
    "val hockeyTF = hockeyText.mapValues( doc => hashingTF.transform(tokenize(doc)))\n",
    "val hockeyTfIdf = idf.transform( hockeyTF.map(_._2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.060250114361164626\n"
     ]
    }
   ],
   "source": [
    "import breeze.linalg._\n",
    "val hockey1 = hockeyTfIdf.sample( true, 0.1, 42).first.asInstanceOf[SV]\n",
    "val breeze1 = new SparseVector( hockey1.indices, hockey1.values, hockey1.size )\n",
    "val hockey2 = hockeyTfIdf.sample( true, 0.1, 43).first.asInstanceOf[SV]\n",
    "val breeze2 = new SparseVector( hockey2.indices, hockey2.values, hockey2.size )\n",
    "val cosineSim = breeze1.dot( breeze2 ) / ( norm( breeze1 ) * norm( breeze2 ) )\n",
    "println( cosineSim )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004664850323792852\n"
     ]
    }
   ],
   "source": [
    "val graphicsText = rdd.filter { case( file, text) => file.contains(\"comp.graphics\")}\n",
    "val graphicsTF = graphicsText.mapValues( doc => hashingTF.transform(tokenize(doc)))\n",
    "val graphicsTfIdf = idf.transform( graphicsTF.map( _._2))\n",
    "val graphics = graphicsTfIdf.sample( true, 0.1, 42 ).first.asInstanceOf[SV]\n",
    "val breezeGraphics = new SparseVector( graphics.indices, graphics.values, graphics.size )\n",
    "val cosineSim2 = breeze1.dot( breezeGraphics) / ( norm(breeze1) * norm(breezeGraphics))\n",
    "println( cosineSim2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05047395039466008\n"
     ]
    }
   ],
   "source": [
    "val baseballText = rdd.filter{ case(file, text) => file.contains(\"baseball\") }\n",
    "val baseballTF = baseballText.mapValues( doc => hashingTF.transform(tokenize(doc)))\n",
    "val baseballTfIdf = idf.transform( baseballTF.map(_._2))\n",
    "val baseball = baseballTfIdf.sample( true, 0.1, 42).first.asInstanceOf[SV]\n",
    "val breezeBaseball = new SparseVector(baseball.indices, baseball.values, baseball.size)\n",
    "val cosineSim3 = breeze1.dot( breezeBaseball ) / (norm(breeze1) * norm(breezeBaseball))\n",
    "println( cosineSim3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  基于20 newsgroups 数据集使用TF-IDF训练文本分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[94] at map at <console>:61"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.classification.NaiveBayes\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "\n",
    "val newsgroupsMap = newsgroups.distinct.collect().zipWithIndex.toMap\n",
    "val zipped = newsgroups.zip(tfidf)\n",
    "val train = zipped.map{ case(topic, vector) => LabeledPoint(newsgroupsMap(topic), vector)}\n",
    "train.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val model = NaiveBayes.train( train, lambda = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val testRDD = sc.wholeTextFiles( PATH + \"data/20news-bydate-test/*\" )\n",
    "val testLabels = testRDD.map{ case(file, text) => \n",
    "    val topic = file.split(\"/\").takeRight(2).head\n",
    "    newsgroupsMap( topic )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7915560276155071\n",
      "0.7810675969031116\n"
     ]
    }
   ],
   "source": [
    "val predictionAndLabel = test.map( p => (model.predict(p.features), p.label))\n",
    "val accuracy = 1.0 * predictionAndLabel.filter( x => x._1 == x._2 ).count() / test.count()\n",
    "val metrics = new MulticlassMetrics( predictionAndLabel )\n",
    "println( accuracy )\n",
    "println( metrics.weightedFMeasure )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val testTf = testRDD.map{ case(file, text) =>\n",
    "    hashingTF.transform( tokenize(text))\n",
    "}\n",
    "val testTfIdf = idf.transform( testTf )\n",
    "val zippedTest = testLabels.zip( testTfIdf )\n",
    "val test = zippedTest.map{ case( topic, vector) => LabeledPoint(topic, vector) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  评估文本处理技术等作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val rawTokens = rdd.map{ case(file, text) => text.split(\" \") }\n",
    "val rawTF = rawTokens.map( doc => hashingTF.transform(doc))\n",
    "val rawTrain = newsgroups.zip(rawTF).map{ case( topic, vector ) => LabeledPoint( newsgroupsMap(topic), vector)}\n",
    "val rawModel = NaiveBayes.train(rawTrain, lambda = 0.1 )\n",
    "val rawTestTF = testRDD.map{ case(file, text) => hashingTF.transform(text.split(\" \")) }\n",
    "val rawZippedTest = testLabels.zip( rawTestTF )\n",
    "val rawTest = rawZippedTest.map{ case (topic, vector) => LabeledPoint(topic, vector)}\n",
    "val rawPredictionAndLabel = rawTest.map( p => (rawModel.predict(p.features), p.label))\n",
    "val rawAccuracy = 1.0 * rawPredictionAndLabel.filter( x => x._1 == x._2 ).count() / rawTest.count()\n",
    "println( rawAccuracy)\n",
    "val rawMetrics = new MulticlassMetrics(rawPredictionAndLabel)\n",
    "println(rawMetrics.weightedFMeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.feature.Word2Vec\n",
    "val word2vect = new Word2Vec()\n",
    "word2vec.setSeed(42)\n",
    "val word2vecModel = word2vec.fit(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vecModel.findSynonyms(\"hockey\", 20).foreach( println )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vecModel.findSynonyms(\"legislation\", 20).foreach(println)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.5.2 (Scala 2.10)",
   "language": "",
   "name": "spark"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
