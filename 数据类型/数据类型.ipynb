{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在单机模式下机器学习库支持本地向量和矩阵存储，以及分布式矩阵由一个或多个RDD组成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local vector(本地向量)单机模式下支持两种本地向量dense 和 sparse\n",
    "密集向量由一组double值组成\n",
    "稀疏向量有两种表示形式1［（一组索引，一组值）］2［（索引，值），（索引，值）］"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,[0,2],[1.0,3.0])\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "\n",
    "// Create a dense vector (1.0, 0.0, 3.0).\n",
    "val dv: Vector = Vectors.dense(1.0, 0.0, 3.0)\n",
    "// Create a sparse vector (1.0, 0.0, 3.0) by specifying its indices and values corresponding to nonzero entries.\n",
    "val sv1: Vector = Vectors.sparse(3, Array(0, 2), Array(1.0, 3.0))\n",
    "// Create a sparse vector (1.0, 0.0, 3.0) by specifying its nonzero entries.\n",
    "val sv2: Vector = Vectors.sparse(3, Seq((0, 1.0), (2, 3.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeled point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0,(3,[0,2],[1.0,3.0]))\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "\n",
    "// Create a labeled point with a positive label and a dense feature vector.\n",
    "val pos = LabeledPoint(1.0, Vectors.dense(1.0, 0.0, 3.0))\n",
    "\n",
    "// Create a labeled point with a negative label and a sparse feature vector.\n",
    "val neg = LabeledPoint(0.0, Vectors.sparse(3, Array(0, 2), Array(1.0, 3.0)))\n",
    "println(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((0.0,(692,[127,128,129,130,131,154,155,156,157,158,159,181,182,183,184,185,186,187,188,189,207,208,209,210,211,212,213,214,215,216,217,235,236,237,238,239,240,241,242,243,244,245,262,263,264,265,266,267,268,269,270,271,272,273,289,290,291,292,293,294,295,296,297,300,301,302,316,317,318,319,320,321,328,329,330,343,344,345,346,347,348,349,356,357,358,371,372,373,374,384,385,386,399,400,401,412,413,414,426,427,428,429,440,441,442,454,455,456,457,466,467,468,469,470,482,483,484,493,494,495,496,497,510,511,512,520,521,522,523,538,539,540,547,548,549,550,566,567,568,569,570,571,572,573,574,575,576,577,578,594,595,596,597,598,599,600,601,602,603,604,622,623,624,625,626,627,628,629,630,651,652,653,654,655,656,657],[51.0,159.0,..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "val examples: RDD[LabeledPoint] = MLUtils.loadLibSVMFile(sc, \"../data/mllib/sample_libsvm_data.txt\")\n",
    "examples.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local matrix\n",
    "$$x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}$$\n",
    "\n",
    "$$\\begin{pmatrix}\n",
    "1.0 & 2.0 \\\\\n",
    "3.0 & 4.0 \\\\\n",
    "5.0 & 6.0\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.5.2 (Scala 2.10)",
   "language": "",
   "name": "spark"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
