{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val PATH = \"file:///Users/lzz/work/SparkML/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\begin{equation}\n",
    "    f(v) := \\lambda\\, R(v) +\n",
    "    \\frac1n \\sum_{i=1}^n L(v;x_i,y_i)\n",
    "    \\label{eq:regPrimal}\n",
    "    \\ .\n",
    "\\end{equation} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " |      loss function L(v; x, y)       ..... |         gradient or sub-gradient      .......\n",
    " --------------------- | ------------------------- | ----------------------------\n",
    "hinge loss | $ \\max \\{0, 1-y w^T x \\}, \\quad y \\in \\{-1, +1\\} $ | $ \\begin{cases}-y \\cdot x & \\text{if $y w^T x <1$}, \\\\ 0 &\n",
    "\\text{otherwise}.\\end{cases} $\n",
    "logistic loss | $ \\log(1+\\exp( -y w^T x)), \\quad y \\in \\{-1, +1\\} $  | $ -y \\left(1-\\frac1{1+\\exp(-y w^T x)} \\right) \\cdot x $\n",
    "squared loss | $ \\frac{1}{2} (w^T x - y)^2, \\quad y \\in R $ | $ ( w^T x - y) \\cdot x $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " ......|      regularizer R(w)    .........     ..    .....|         gradient or sub-gradient  .......   ..   \n",
    " --------------------- | ------------------------- | ----------------------------\n",
    " zero (unregularized) | 0 | $ 0 $\n",
    " L2 | $ \\frac{1}{2}\\|w\\|_2^2 $ | $ w $\n",
    " L1 | $ \\|w\\|_1 $ | $ \\mathrm{sign}(w) $\n",
    " elastic net | $ \\alpha \\|w\\|_1 + (1-\\alpha)\\frac{1}{2}\\|w\\|_2^2 $ | $ \\alpha \\mathrm{sign}(w) + (1-\\alpha) w $\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Linear Support Vector Machines (SVMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM 使用的是hinge loss是损失函数，默认的正则规则是L2，我们也可以设置成L1\n",
    "$ L(w;x,y) := \\max \\{0, 1-y w^T x \\}. $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC = 1.0\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "// Load training data in LIBSVM format.\n",
    "val data = MLUtils.loadLibSVMFile(sc, PATH + \"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "// Split data into training (60%) and test (40%).\n",
    "val splits = data.randomSplit(Array(0.6, 0.4), seed = 11L)\n",
    "val training = splits(0).cache()\n",
    "val test = splits(1)\n",
    "\n",
    "// Run training algorithm to build the model\n",
    "val numIterations = 100\n",
    "val model = SVMWithSGD.train(training, numIterations)\n",
    "\n",
    "// Clear the default threshold.\n",
    "model.clearThreshold()\n",
    "\n",
    "// Compute raw scores on the test set.\n",
    "val scoreAndLabels = test.map { point =>\n",
    "  val score = model.predict(point.features)\n",
    "  (score, point.label)\n",
    "}\n",
    "\n",
    "// Get evaluation metrics.\n",
    "val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "val auROC = metrics.areaUnderROC()\n",
    "\n",
    "println(\"Area under ROC = \" + auROC)\n",
    "\n",
    "// Save and load model\n",
    "model.save(sc, \"myModelPath\")\n",
    "val sameModel = SVMModel.load(sc, \"myModelPath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.optimization.L1Updater\n",
    "\n",
    "val svmAlg = new SVMWithSGD()\n",
    "svmAlg.optimizer.\n",
    "  setNumIterations(200).\n",
    "  setRegParam(0.1).\n",
    "  setUpdater(new L1Updater)\n",
    "val modelL1 = svmAlg.run(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逻辑回归使用的是logistic loss 损失函数，\n",
    "\n",
    "$ L(w;x,y) :=  \\log(1+\\exp( -y w^T x)). $\n",
    "\n",
    "当−ywTx <＝0 的时候 exp(−ywTx) <＝ 1， 所以log(1) <= 0,这个就是函数为什么要加1点原因"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 1.0\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.mllib.classification.{LogisticRegressionWithLBFGS, LogisticRegressionModel}\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "// Load training data in LIBSVM format.\n",
    "val data = MLUtils.loadLibSVMFile(sc, PATH + \"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "// Split data into training (60%) and test (40%).\n",
    "val splits = data.randomSplit(Array(0.6, 0.4), seed = 11L)\n",
    "val training = splits(0).cache()\n",
    "val test = splits(1)\n",
    "\n",
    "// Run training algorithm to build the model\n",
    "val model = new LogisticRegressionWithLBFGS().setNumClasses(10).run(training)\n",
    "\n",
    "// Compute raw scores on the test set.\n",
    "val predictionAndLabels = test.map { case LabeledPoint(label, features) =>\n",
    "  val prediction = model.predict(features)\n",
    "  (prediction, label)\n",
    "}\n",
    "\n",
    "// Get evaluation metrics.\n",
    "val metrics = new MulticlassMetrics(predictionAndLabels)\n",
    "val precision = metrics.precision\n",
    "println(\"Precision = \" + precision)\n",
    "\n",
    "// Save and load model\n",
    "model.save(sc, \"myModelPath\")\n",
    "val sameModel = LogisticRegressionModel.load(sc, \"myModelPath\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Mean Squared Error = 6.207597210613578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.hadoop.mapred.FileAlreadyExistsException\n",
       "Message: Output directory hdfs://localhost:9000/user/lzz/myModelPath/metadata already exists\n",
       "StackTrace: org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\n",
       "org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1089)\n",
       "org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1065)\n",
       "org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1065)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n",
       "org.apache.spark.rdd.RDD.withScope(RDD.scala:310)\n",
       "org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1065)\n",
       "org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:989)\n",
       "org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:965)\n",
       "org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:965)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n",
       "org.apache.spark.rdd.RDD.withScope(RDD.scala:310)\n",
       "org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:965)\n",
       "org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:897)\n",
       "org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:897)\n",
       "org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:897)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n",
       "org.apache.spark.rdd.RDD.withScope(RDD.scala:310)\n",
       "org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:896)\n",
       "org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1430)\n",
       "org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1409)\n",
       "org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1409)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n",
       "org.apache.spark.rdd.RDD.withScope(RDD.scala:310)\n",
       "org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1409)\n",
       "org.apache.spark.mllib.regression.impl.GLMRegressionModel$SaveLoadV1_0$.save(GLMRegressionModel.scala:57)\n",
       "org.apache.spark.mllib.regression.LinearRegressionModel.save(LinearRegression.scala:52)\n",
       "$line199.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:71)\n",
       "$line199.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:76)\n",
       "$line199.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:78)\n",
       "$line199.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:80)\n",
       "$line199.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:82)\n",
       "$line199.$read$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:84)\n",
       "$line199.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:86)\n",
       "$line199.$read$$iwC$$iwC$$iwC.<init>(<console>:88)\n",
       "$line199.$read$$iwC$$iwC.<init>(<console>:90)\n",
       "$line199.$read$$iwC.<init>(<console>:92)\n",
       "$line199.$read.<init>(<console>:94)\n",
       "$line199.$read$.<init>(<console>:98)\n",
       "$line199.$read$.<clinit>(<console>)\n",
       "$line199.$eval$.<init>(<console>:7)\n",
       "$line199.$eval$.<clinit>(<console>)\n",
       "$line199.$eval.$print(<console>)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
       "org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340)\n",
       "org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:361)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:356)\n",
       "org.apache.toree.global.StreamState$.withStreams(StreamState.scala:81)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
       "org.apache.toree.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:140)\n",
       "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.regression.LinearRegressionModel\n",
    "import org.apache.spark.mllib.regression.LinearRegressionWithSGD\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "// Load and parse the data\n",
    "val data = sc.textFile( PATH + \"data/mllib/ridge-data/lpsa.data\")\n",
    "val parsedData = data.map { line =>\n",
    "  val parts = line.split(',')\n",
    "  LabeledPoint(parts(0).toDouble, Vectors.dense(parts(1).split(' ').map(_.toDouble)))\n",
    "}.cache()\n",
    "\n",
    "// Building the model\n",
    "val numIterations = 100\n",
    "val model = LinearRegressionWithSGD.train(parsedData, numIterations)\n",
    "\n",
    "// Evaluate model on training examples and compute training error\n",
    "val valuesAndPreds = parsedData.map { point =>\n",
    "  val prediction = model.predict(point.features)\n",
    "  (point.label, prediction)\n",
    "}\n",
    "val MSE = valuesAndPreds.map{case(v, p) => math.pow((v - p), 2)}.mean()\n",
    "println(\"training Mean Squared Error = \" + MSE)\n",
    "\n",
    "// Save and load model\n",
    "model.save(sc, \"myModelPath\")\n",
    "val sameModel = LinearRegressionModel.load(sc, \"myModelPath\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.5.2 (Scala 2.10)",
   "language": "",
   "name": "spark"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
