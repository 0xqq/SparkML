{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FP-growth\n",
    "The FP-growth algorithm is described in the paper Han et al., Mining frequent patterns without candidate generation, where “FP” stands for frequent pattern. Given a dataset of transactions, the first step of FP-growth is to calculate item frequencies and identify frequent items. Different from Apriori-like algorithms designed for the same purpose, the second step of FP-growth uses a suffix tree (FP-tree) structure to encode transactions without generating candidate sets explicitly, which are usually expensive to generate. After the second step, the frequent itemsets can be extracted from the FP-tree. In MLlib, we implemented a parallel version of FP-growth called PFP, as described in Li et al., PFP: Parallel FP-growth for query recommendation. PFP distributes the work of growing FP-trees based on the suffices of transactions, and hence more scalable than a single-machine implementation. We refer users to the papers for more details.  \n",
    "\n",
    "MLlib’s FP-growth implementation takes the following (hyper-)parameters:\n",
    "\n",
    "* minSupport: the minimum support for an itemset to be identified as frequent. For example, if an item appears 3 out of 5 transactions, it has a support of 3/5=0.6.\n",
    "* numPartitions: the number of partitions used to distribute the work.  \n",
    "\n",
    "## Examples\n",
    "FPGrowth implements the FP-growth algorithm. It take a RDD of transactions, where each transaction is an Array of items of a generic type. Calling FPGrowth.run with transactions returns an FPGrowthModel that stores the frequent itemsets with their frequencies. The following example illustrates how to mine frequent itemsets and association rules (see Association Rules for details) from transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[z], 5\n",
      "[x], 4\n",
      "[x,z], 3\n",
      "[y], 3\n",
      "[y,x], 3\n",
      "[y,x,z], 3\n",
      "[y,z], 3\n",
      "[r], 3\n",
      "[r,x], 2\n",
      "[r,z], 2\n",
      "[s], 3\n",
      "[s,y], 2\n",
      "[s,y,x], 2\n",
      "[s,y,x,z], 2\n",
      "[s,y,z], 2\n",
      "[s,x], 3\n",
      "[s,x,z], 2\n",
      "[s,z], 2\n",
      "[t], 3\n",
      "[t,y], 3\n",
      "[t,y,x], 3\n",
      "[t,y,x,z], 3\n",
      "[t,y,z], 3\n",
      "[t,s], 2\n",
      "[t,s,y], 2\n",
      "[t,s,y,x], 2\n",
      "[t,s,y,x,z], 2\n",
      "[t,s,y,z], 2\n",
      "[t,s,x], 2\n",
      "[t,s,x,z], 2\n",
      "[t,s,z], 2\n",
      "[t,x], 3\n",
      "[t,x,z], 3\n",
      "[t,z], 3\n",
      "[p], 2\n",
      "[p,r], 2\n",
      "[p,r,z], 2\n",
      "[p,z], 2\n",
      "[q], 2\n",
      "[q,y], 2\n",
      "[q,y,x], 2\n",
      "[q,y,x,z], 2\n",
      "[q,y,z], 2\n",
      "[q,t], 2\n",
      "[q,t,y], 2\n",
      "[q,t,y,x], 2\n",
      "[q,t,y,x,z], 2\n",
      "[q,t,y,z], 2\n",
      "[q,t,x], 2\n",
      "[q,t,x,z], 2\n",
      "[q,t,z], 2\n",
      "[q,x], 2\n",
      "[q,x,z], 2\n",
      "[q,z], 2\n",
      "[t,s,y] => [x], 1.0\n",
      "[t,s,y] => [z], 1.0\n",
      "[y,x,z] => [t], 1.0\n",
      "[y] => [x], 1.0\n",
      "[y] => [z], 1.0\n",
      "[y] => [t], 1.0\n",
      "[p] => [r], 1.0\n",
      "[p] => [z], 1.0\n",
      "[q,t,z] => [y], 1.0\n",
      "[q,t,z] => [x], 1.0\n",
      "[q,y] => [x], 1.0\n",
      "[q,y] => [z], 1.0\n",
      "[q,y] => [t], 1.0\n",
      "[t,s,x] => [y], 1.0\n",
      "[t,s,x] => [z], 1.0\n",
      "[q,t,y,z] => [x], 1.0\n",
      "[q,t,x,z] => [y], 1.0\n",
      "[q,x] => [y], 1.0\n",
      "[q,x] => [t], 1.0\n",
      "[q,x] => [z], 1.0\n",
      "[t,x,z] => [y], 1.0\n",
      "[x,z] => [y], 1.0\n",
      "[x,z] => [t], 1.0\n",
      "[p,z] => [r], 1.0\n",
      "[t] => [y], 1.0\n",
      "[t] => [x], 1.0\n",
      "[t] => [z], 1.0\n",
      "[y,z] => [x], 1.0\n",
      "[y,z] => [t], 1.0\n",
      "[p,r] => [z], 1.0\n",
      "[t,s] => [y], 1.0\n",
      "[t,s] => [x], 1.0\n",
      "[t,s] => [z], 1.0\n",
      "[q,z] => [y], 1.0\n",
      "[q,z] => [t], 1.0\n",
      "[q,z] => [x], 1.0\n",
      "[q,y,z] => [x], 1.0\n",
      "[q,y,z] => [t], 1.0\n",
      "[y,x] => [z], 1.0\n",
      "[y,x] => [t], 1.0\n",
      "[q,x,z] => [y], 1.0\n",
      "[q,x,z] => [t], 1.0\n",
      "[t,y,z] => [x], 1.0\n",
      "[q,y,x] => [z], 1.0\n",
      "[q,y,x] => [t], 1.0\n",
      "[q,t,y,x] => [z], 1.0\n",
      "[t,s,x,z] => [y], 1.0\n",
      "[s,y,x] => [z], 1.0\n",
      "[s,y,x] => [t], 1.0\n",
      "[s,x,z] => [y], 1.0\n",
      "[s,x,z] => [t], 1.0\n",
      "[q,y,x,z] => [t], 1.0\n",
      "[s,y] => [x], 1.0\n",
      "[s,y] => [z], 1.0\n",
      "[s,y] => [t], 1.0\n",
      "[q,t,y] => [x], 1.0\n",
      "[q,t,y] => [z], 1.0\n",
      "[t,y] => [x], 1.0\n",
      "[t,y] => [z], 1.0\n",
      "[t,z] => [y], 1.0\n",
      "[t,z] => [x], 1.0\n",
      "[t,s,y,x] => [z], 1.0\n",
      "[t,y,x] => [z], 1.0\n",
      "[q,t] => [y], 1.0\n",
      "[q,t] => [x], 1.0\n",
      "[q,t] => [z], 1.0\n",
      "[q] => [y], 1.0\n",
      "[q] => [t], 1.0\n",
      "[q] => [x], 1.0\n",
      "[q] => [z], 1.0\n",
      "[t,s,z] => [y], 1.0\n",
      "[t,s,z] => [x], 1.0\n",
      "[t,x] => [y], 1.0\n",
      "[t,x] => [z], 1.0\n",
      "[s,z] => [y], 1.0\n",
      "[s,z] => [x], 1.0\n",
      "[s,z] => [t], 1.0\n",
      "[s,y,x,z] => [t], 1.0\n",
      "[s] => [x], 1.0\n",
      "[t,s,y,z] => [x], 1.0\n",
      "[s,y,z] => [x], 1.0\n",
      "[s,y,z] => [t], 1.0\n",
      "[q,t,x] => [y], 1.0\n",
      "[q,t,x] => [z], 1.0\n",
      "[r,z] => [p], 1.0\n"
     ]
    }
   ],
   "source": [
    "val PATH = \"file:///Users/lzz/work/SparkML/\"\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.mllib.fpm.FPGrowth\n",
    "\n",
    "val data = sc.textFile(PATH+\"data/mllib/sample_fpgrowth.txt\")\n",
    "\n",
    "val transactions: RDD[Array[String]] = data.map(s => s.trim.split(' '))\n",
    "\n",
    "val fpg = new FPGrowth().setMinSupport(0.2).setNumPartitions(10)\n",
    "val model = fpg.run(transactions)\n",
    "\n",
    "model.freqItemsets.collect().foreach { itemset =>\n",
    "  println(itemset.items.mkString(\"[\", \",\", \"]\") + \", \" + itemset.freq)\n",
    "}\n",
    "\n",
    "val minConfidence = 0.8\n",
    "model.generateAssociationRules(minConfidence).collect().foreach { rule =>\n",
    "  println(\n",
    "    rule.antecedent.mkString(\"[\", \",\", \"]\")\n",
    "      + \" => \" + rule.consequent .mkString(\"[\", \",\", \"]\")\n",
    "      + \", \" + rule.confidence)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.5.2 (Scala 2.10)",
   "language": "",
   "name": "spark"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
