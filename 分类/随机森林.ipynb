{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "The example below demonstrates how to load a LIBSVM data file, parse it as an RDD of LabeledPoint and then perform classification using a Random Forest. The test error is calculated to measure the algorithm accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.0\n",
      "Learned classification forest model:\n",
      "TreeEnsembleModel classifier with 3 trees\n",
      "\n",
      "  Tree 0:\n",
      "    If (feature 344 <= 0.0)\n",
      "     If (feature 378 <= 71.0)\n",
      "      Predict: 0.0\n",
      "     Else (feature 378 > 71.0)\n",
      "      Predict: 1.0\n",
      "    Else (feature 344 > 0.0)\n",
      "     If (feature 523 <= 31.0)\n",
      "      If (feature 688 <= 0.0)\n",
      "       Predict: 1.0\n",
      "      Else (feature 688 > 0.0)\n",
      "       Predict: 0.0\n",
      "     Else (feature 523 > 31.0)\n",
      "      Predict: 0.0\n",
      "  Tree 1:\n",
      "    If (feature 433 <= 0.0)\n",
      "     If (feature 324 <= 38.0)\n",
      "      Predict: 0.0\n",
      "     Else (feature 324 > 38.0)\n",
      "      Predict: 1.0\n",
      "    Else (feature 433 > 0.0)\n",
      "     Predict: 1.0\n",
      "  Tree 2:\n",
      "    If (feature 463 <= 0.0)\n",
      "     If (feature 317 <= 0.0)\n",
      "      If (feature 489 <= 0.0)\n",
      "       Predict: 0.0\n",
      "      Else (feature 489 > 0.0)\n",
      "       Predict: 1.0\n",
      "     Else (feature 317 > 0.0)\n",
      "      Predict: 0.0\n",
      "    Else (feature 463 > 0.0)\n",
      "     Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val PATH = \"file:///Users/lzz/work/SparkML/\"\n",
    "\n",
    "import org.apache.spark.mllib.tree.RandomForest\n",
    "import org.apache.spark.mllib.tree.model.RandomForestModel\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "// Load and parse the data file.\n",
    "val data = MLUtils.loadLibSVMFile(sc, PATH + \"data/mllib/sample_libsvm_data.txt\")\n",
    "// Split the data into training and test sets (30% held out for testing)\n",
    "val splits = data.randomSplit(Array(0.7, 0.3))\n",
    "val (trainingData, testData) = (splits(0), splits(1))\n",
    "\n",
    "// Train a RandomForest model.\n",
    "//  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "val numClasses = 2\n",
    "val categoricalFeaturesInfo = Map[Int, Int]()\n",
    "val numTrees = 3 // Use more in practice.\n",
    "val featureSubsetStrategy = \"auto\" // Let the algorithm choose.\n",
    "val impurity = \"gini\"\n",
    "val maxDepth = 4\n",
    "val maxBins = 32\n",
    "\n",
    "val model = RandomForest.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,\n",
    "  numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)\n",
    "\n",
    "// Evaluate model on test instances and compute test error\n",
    "val labelAndPreds = testData.map { point =>\n",
    "  val prediction = model.predict(point.features)\n",
    "  (point.label, prediction)\n",
    "}\n",
    "val testErr = labelAndPreds.filter(r => r._1 != r._2).count.toDouble / testData.count()\n",
    "println(\"Test Error = \" + testErr)\n",
    "println(\"Learned classification forest model:\\n\" + model.toDebugString)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "The example below demonstrates how to load a LIBSVM data file, parse it as an RDD of LabeledPoint and then perform regression using a Random Forest. The Mean Squared Error (MSE) is computed at the end to evaluate goodness of fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Mean Squared Error = 0.03831417624521073\n",
      "Learned regression forest model:\n",
      "TreeEnsembleModel regressor with 3 trees\n",
      "\n",
      "  Tree 0:\n",
      "    If (feature 489 <= 0.0)\n",
      "     Predict: 0.0\n",
      "    Else (feature 489 > 0.0)\n",
      "     Predict: 1.0\n",
      "  Tree 1:\n",
      "    If (feature 490 <= 31.0)\n",
      "     Predict: 0.0\n",
      "    Else (feature 490 > 31.0)\n",
      "     Predict: 1.0\n",
      "  Tree 2:\n",
      "    If (feature 490 <= 31.0)\n",
      "     Predict: 0.0\n",
      "    Else (feature 490 > 31.0)\n",
      "     Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.tree.RandomForest\n",
    "import org.apache.spark.mllib.tree.model.RandomForestModel\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "// Load and parse the data file.\n",
    "val data = MLUtils.loadLibSVMFile(sc, PATH + \"data/mllib/sample_libsvm_data.txt\")\n",
    "// Split the data into training and test sets (30% held out for testing)\n",
    "val splits = data.randomSplit(Array(0.7, 0.3))\n",
    "val (trainingData, testData) = (splits(0), splits(1))\n",
    "\n",
    "// Train a RandomForest model.\n",
    "//  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "val numClasses = 2\n",
    "val categoricalFeaturesInfo = Map[Int, Int]()\n",
    "val numTrees = 3 // Use more in practice.\n",
    "val featureSubsetStrategy = \"auto\" // Let the algorithm choose.\n",
    "val impurity = \"variance\"\n",
    "val maxDepth = 4\n",
    "val maxBins = 32\n",
    "\n",
    "val model = RandomForest.trainRegressor(trainingData, categoricalFeaturesInfo,\n",
    "  numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)\n",
    "\n",
    "// Evaluate model on test instances and compute test error\n",
    "val labelsAndPredictions = testData.map { point =>\n",
    "  val prediction = model.predict(point.features)\n",
    "  (point.label, prediction)\n",
    "}\n",
    "val testMSE = labelsAndPredictions.map{ case(v, p) => math.pow((v - p), 2)}.mean()\n",
    "println(\"Test Mean Squared Error = \" + testMSE)\n",
    "println(\"Learned regression forest model:\\n\" + model.toDebugString)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.5.2 (Scala 2.10)",
   "language": "",
   "name": "spark"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
