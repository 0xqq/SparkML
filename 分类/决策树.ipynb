{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树（Decision Trees）\n",
    "在机器学习算法中决策树和它的集成方法（随机森林,GBDT）都是很流行的分类和回归方法。决策树在很多任务上表现出的性能很好，相对容易解释和理解，可以处理类属或者数值特征，同时不要求数据归一化或标准化。决策树非常适用集成方法（ensemble method）,比如多个决策树的集成，称为决策树森林。  \n",
    "## 算法基础\n",
    "决策树是一种贪婪算法，通过递归二分类特征空间。为了在下一个子树获得最大信息增益，每个分区都尽可能地选择最佳分裂点。换句话讲对于数据集D我们要找到一个分隔点s使得信息增益$\\underset{s}{\\operatorname{argmax}} IG(D,s)$最大。\n",
    "## 节点混乱度（impurity）和信息增益（information gain）\n",
    "节点点混乱度是衡量节点上标签点均匀度。目前MLlib提供了两种度量分类混乱度的方法(Gini impurity and entropy)和一种回归混乱度的度量的方法（variance）。\n",
    "\n",
    "|    Impurity   |   Task   |  Formula  | Description |\n",
    "| ---- | ---- | ---- |\n",
    "| Gini impurity | Classification | $\\sum_{i=1}^{C} f_i(1-f_i)$ | $f_i$is the frequency of label ii at a node and CC is the number of unique labels. |\n",
    "| Entropy | Classification | $\\sum_{i=1}^{C} -f_ilog(f_i)$ | $f_i$ is the frequency of label ii at a node and CC is the number of unique labels.(对于单个分类的impurity $l(x_i)=-log(f_i)$,表示选择该分类的概率越大混乱度$l(x_i)$越小。概率越小就不能保证混乱度越大了，因为没办法保证其它分类的概率分布  ) |\n",
    "| Variance | Regression | $\\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\mu)^2$ | $y_i$is label for an instance, NN is the number of instances and μμ is the mean given by $\\frac{1}{N} \\sum_{i=1}^N y_i$ |  \n",
    "信息增益是父节点和两个子节点混乱度加权总和的差异。假设分割点s将大小为N的数据集D分割成两部分$D_{left}$和$D_{right}$大小分别为$N_{left}$和$N_{right}$,所以信息增益有：  \n",
    "$IG(D,s) = Impurity(D) - \\frac{N_{left}}{N} Impurity(D_{left}) - \\frac{N_{right}}{N} Impurity(D_{right})$\n",
    "## 分割点的选择（Split candidates）\n",
    "### 连续型特征（Continuous features）\n",
    "对于在单机实现中的小数据集，每个连续特征的分割候选通常是特征的唯一值。一些实现对特征值进行排序，然后使用排序的唯一值作为更快的树计算的分割候选。\n",
    "对于大型分布式数据集的排序特征值是昂贵的。实现计算的方式是通过设置分割点通过数据采样的分位数计算。有序分割点创建“bins”最大bins可以通过 maxBins 参数设置。\n",
    "### 类别型特征（Categorical features）\n",
    "一个分类特征有M种可能的值那么我们可能要进行$2^{M-1}-1$次的分割候选。\n",
    "## 停止规则（Stopping rule）\n",
    "当满足下列条件之一时，递归树的构建在节点上停止：  \n",
    "1) 节点的深度等于maxDepth训练参数。  \n",
    "2) 分裂的候选没有信息增益大于mininfogain。  \n",
    "3) 到达叶子节点了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.08571428571428572\n",
      "Learned classification tree model:\n",
      "DecisionTreeModel classifier of depth 1 with 3 nodes\n",
      "  If (feature 378 <= 71.0)\n",
      "   Predict: 0.0\n",
      "  Else (feature 378 > 71.0)\n",
      "   Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val PATH = \"file:///Users/lzz/work/SparkML/\"\n",
    "import org.apache.spark.mllib.tree.DecisionTree\n",
    "import org.apache.spark.mllib.tree.model.DecisionTreeModel\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "// Load and parse the data file.\n",
    "val data = MLUtils.loadLibSVMFile(sc, PATH+\"data/mllib/sample_libsvm_data.txt\")\n",
    "// Split the data into training and test sets (30% held out for testing)\n",
    "val splits = data.randomSplit(Array(0.7, 0.3))\n",
    "val (trainingData, testData) = (splits(0), splits(1))\n",
    "\n",
    "// Train a DecisionTree model.\n",
    "//  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "val numClasses = 2\n",
    "val categoricalFeaturesInfo = Map[Int, Int]()\n",
    "val impurity = \"gini\"\n",
    "val maxDepth = 5\n",
    "val maxBins = 32\n",
    "\n",
    "val model = DecisionTree.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,\n",
    "  impurity, maxDepth, maxBins)\n",
    "\n",
    "// Evaluate model on test instances and compute test error\n",
    "val labelAndPreds = testData.map { point =>\n",
    "  val prediction = model.predict(point.features)\n",
    "  (point.label, prediction)\n",
    "}\n",
    "val testErr = labelAndPreds.filter(r => r._1 != r._2).count.toDouble / testData.count()\n",
    "println(\"Test Error = \" + testErr)\n",
    "println(\"Learned classification tree model:\\n\" + model.toDebugString)\n",
    "\n",
    "// Save and load model\n",
    "model.save(sc, \"myModelPath\")\n",
    "val sameModel = DecisionTreeModel.load(sc, \"myModelPath\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regression \n",
    "The example below demonstrates how to load a LIBSVM data file, parse it as an RDD of LabeledPoint and then perform regression using a decision tree with variance as an impurity measure and a maximum tree depth of 5. The Mean Squared Error (MSE) is computed at the end to evaluate goodness of fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Mean Squared Error = 0.05555555555555555\n",
      "Learned regression tree model:\n",
      "DecisionTreeModel regressor of depth 1 with 3 nodes\n",
      "  If (feature 406 <= 0.0)\n",
      "   Predict: 0.0\n",
      "  Else (feature 406 > 0.0)\n",
      "   Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.tree.DecisionTree\n",
    "import org.apache.spark.mllib.tree.model.DecisionTreeModel\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "// Load and parse the data file.\n",
    "val data = MLUtils.loadLibSVMFile(sc, PATH + \"data/mllib/sample_libsvm_data.txt\")\n",
    "// Split the data into training and test sets (30% held out for testing)\n",
    "val splits = data.randomSplit(Array(0.7, 0.3))\n",
    "val (trainingData, testData) = (splits(0), splits(1))\n",
    "\n",
    "// Train a DecisionTree model.\n",
    "//  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "val categoricalFeaturesInfo = Map[Int, Int]()\n",
    "val impurity = \"variance\"\n",
    "val maxDepth = 5\n",
    "val maxBins = 32\n",
    "\n",
    "val model = DecisionTree.trainRegressor(trainingData, categoricalFeaturesInfo, impurity,\n",
    "  maxDepth, maxBins)\n",
    "\n",
    "// Evaluate model on test instances and compute test error\n",
    "val labelsAndPredictions = testData.map { point =>\n",
    "  val prediction = model.predict(point.features)\n",
    "  (point.label, prediction)\n",
    "}\n",
    "val testMSE = labelsAndPredictions.map{ case(v, p) => math.pow((v - p), 2)}.mean()\n",
    "println(\"Test Mean Squared Error = \" + testMSE)\n",
    "println(\"Learned regression tree model:\\n\" + model.toDebugString)\n",
    "\n",
    "// Save and load model\n",
    "model.save(sc, \"myModelPath\")\n",
    "val sameModel = DecisionTreeModel.load(sc, \"myModelPath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.5.2 (Scala 2.10)",
   "language": "",
   "name": "spark"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
