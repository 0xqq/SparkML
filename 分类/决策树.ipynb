{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.06451612903225806\n",
      "Learned classification tree model:\n",
      "DecisionTreeModel classifier of depth 1 with 3 nodes\n",
      "  If (feature 406 <= 0.0)\n",
      "   Predict: 0.0\n",
      "  Else (feature 406 > 0.0)\n",
      "   Predict: 1.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.hadoop.mapred.FileAlreadyExistsException\n",
       "Message: Output directory hdfs://localhost:9000/user/lzz/myModelPath/metadata already exists\n",
       "StackTrace: org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\n",
       "org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1089)\n",
       "org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1065)\n",
       "org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1065)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n",
       "org.apache.spark.rdd.RDD.withScope(RDD.scala:310)\n",
       "org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1065)\n",
       "org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:989)\n",
       "org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:965)\n",
       "org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:965)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n",
       "org.apache.spark.rdd.RDD.withScope(RDD.scala:310)\n",
       "org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:965)\n",
       "org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:897)\n",
       "org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:897)\n",
       "org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:897)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n",
       "org.apache.spark.rdd.RDD.withScope(RDD.scala:310)\n",
       "org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:896)\n",
       "org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1430)\n",
       "org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1409)\n",
       "org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1409)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n",
       "org.apache.spark.rdd.RDD.withScope(RDD.scala:310)\n",
       "org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1409)\n",
       "org.apache.spark.mllib.tree.model.DecisionTreeModel$SaveLoadV1_0$.save(DecisionTreeModel.scala:235)\n",
       "org.apache.spark.mllib.tree.model.DecisionTreeModel.save(DecisionTreeModel.scala:128)\n",
       "$line27.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:34)\n",
       "$line27.$read$$iwC$$iwC$$iwC.<init>(<console>:39)\n",
       "$line27.$read$$iwC$$iwC.<init>(<console>:41)\n",
       "$line27.$read$$iwC.<init>(<console>:43)\n",
       "$line27.$read.<init>(<console>:45)\n",
       "$line27.$read$.<init>(<console>:49)\n",
       "$line27.$read$.<clinit>(<console>)\n",
       "$line27.$eval$.<init>(<console>:7)\n",
       "$line27.$eval$.<clinit>(<console>)\n",
       "$line27.$eval.$print(<console>)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
       "org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340)\n",
       "org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:361)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:356)\n",
       "org.apache.toree.global.StreamState$.withStreams(StreamState.scala:81)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
       "org.apache.toree.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:140)\n",
       "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val PATH = \"file:///Users/lzz/work/SparkML/\"\n",
    "import org.apache.spark.mllib.tree.DecisionTree\n",
    "import org.apache.spark.mllib.tree.model.DecisionTreeModel\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "// Load and parse the data file.\n",
    "val data = MLUtils.loadLibSVMFile(sc, PATH+\"data/mllib/sample_libsvm_data.txt\")\n",
    "// Split the data into training and test sets (30% held out for testing)\n",
    "val splits = data.randomSplit(Array(0.7, 0.3))\n",
    "val (trainingData, testData) = (splits(0), splits(1))\n",
    "\n",
    "// Train a DecisionTree model.\n",
    "//  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "val numClasses = 2\n",
    "val categoricalFeaturesInfo = Map[Int, Int]()\n",
    "val impurity = \"gini\"\n",
    "val maxDepth = 5\n",
    "val maxBins = 32\n",
    "\n",
    "val model = DecisionTree.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,\n",
    "  impurity, maxDepth, maxBins)\n",
    "\n",
    "// Evaluate model on test instances and compute test error\n",
    "val labelAndPreds = testData.map { point =>\n",
    "  val prediction = model.predict(point.features)\n",
    "  (point.label, prediction)\n",
    "}\n",
    "val testErr = labelAndPreds.filter(r => r._1 != r._2).count.toDouble / testData.count()\n",
    "println(\"Test Error = \" + testErr)\n",
    "println(\"Learned classification tree model:\\n\" + model.toDebugString)\n",
    "\n",
    "// Save and load model\n",
    "model.save(sc, \"myModelPath\")\n",
    "val sameModel = DecisionTreeModel.load(sc, \"myModelPath\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regression \n",
    "The example below demonstrates how to load a LIBSVM data file, parse it as an RDD of LabeledPoint and then perform regression using a decision tree with variance as an impurity measure and a maximum tree depth of 5. The Mean Squared Error (MSE) is computed at the end to evaluate goodness of fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.hadoop.mapred.InvalidInputException\n",
       "Message: Input path does not exist: hdfs://localhost:9000/user/lzz/data/mllib/sample_libsvm_data.txt\n",
       "StackTrace: org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)\n",
       "org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)\n",
       "org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)\n",
       "org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:207)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n",
       "scala.Option.getOrElse(Option.scala:120)\n",
       "org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n",
       "org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n",
       "scala.Option.getOrElse(Option.scala:120)\n",
       "org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n",
       "org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n",
       "scala.Option.getOrElse(Option.scala:120)\n",
       "org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n",
       "org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n",
       "scala.Option.getOrElse(Option.scala:120)\n",
       "org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n",
       "org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n",
       "scala.Option.getOrElse(Option.scala:120)\n",
       "org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n",
       "org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n",
       "scala.Option.getOrElse(Option.scala:120)\n",
       "org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n",
       "org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1007)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n",
       "org.apache.spark.rdd.RDD.withScope(RDD.scala:310)\n",
       "org.apache.spark.rdd.RDD.reduce(RDD.scala:989)\n",
       "org.apache.spark.mllib.util.MLUtils$.loadLibSVMFile(MLUtils.scala:109)\n",
       "org.apache.spark.mllib.util.MLUtils$.loadLibSVMFile(MLUtils.scala:138)\n",
       "org.apache.spark.mllib.util.MLUtils$.loadLibSVMFile(MLUtils.scala:163)\n",
       "$line35.$read$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:23)\n",
       "$line35.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:28)\n",
       "$line35.$read$$iwC$$iwC$$iwC.<init>(<console>:30)\n",
       "$line35.$read$$iwC$$iwC.<init>(<console>:32)\n",
       "$line35.$read$$iwC.<init>(<console>:34)\n",
       "$line35.$read.<init>(<console>:36)\n",
       "$line35.$read$.<init>(<console>:40)\n",
       "$line35.$read$.<clinit>(<console>)\n",
       "$line35.$eval$.<init>(<console>:7)\n",
       "$line35.$eval$.<clinit>(<console>)\n",
       "$line35.$eval.$print(<console>)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
       "org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340)\n",
       "org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:361)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:356)\n",
       "org.apache.toree.global.StreamState$.withStreams(StreamState.scala:81)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
       "org.apache.toree.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:140)\n",
       "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.tree.DecisionTree\n",
    "import org.apache.spark.mllib.tree.model.DecisionTreeModel\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "// Load and parse the data file.\n",
    "val data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_libsvm_data.txt\")\n",
    "// Split the data into training and test sets (30% held out for testing)\n",
    "val splits = data.randomSplit(Array(0.7, 0.3))\n",
    "val (trainingData, testData) = (splits(0), splits(1))\n",
    "\n",
    "// Train a DecisionTree model.\n",
    "//  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "val categoricalFeaturesInfo = Map[Int, Int]()\n",
    "val impurity = \"variance\"\n",
    "val maxDepth = 5\n",
    "val maxBins = 32\n",
    "\n",
    "val model = DecisionTree.trainRegressor(trainingData, categoricalFeaturesInfo, impurity,\n",
    "  maxDepth, maxBins)\n",
    "\n",
    "// Evaluate model on test instances and compute test error\n",
    "val labelsAndPredictions = testData.map { point =>\n",
    "  val prediction = model.predict(point.features)\n",
    "  (point.label, prediction)\n",
    "}\n",
    "val testMSE = labelsAndPredictions.map{ case(v, p) => math.pow((v - p), 2)}.mean()\n",
    "println(\"Test Mean Squared Error = \" + testMSE)\n",
    "println(\"Learned regression tree model:\\n\" + model.toDebugString)\n",
    "\n",
    "// Save and load model\n",
    "model.save(sc, \"myModelPath\")\n",
    "val sameModel = DecisionTreeModel.load(sc, \"myModelPath\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.5.2 (Scala 2.10)",
   "language": "",
   "name": "spark"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
